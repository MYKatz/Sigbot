{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "import collections\n",
    "\n",
    "scripts = open(\"data/scripts.txt\", \"r\", encoding=\"utf-8\")\n",
    "corpus = scripts.read()\n",
    "\n",
    "def create_tables(words):\n",
    "    count = collections.Counter(words).most_common()\n",
    "    dictionary = {}\n",
    "    for word, k in count:\n",
    "        dictionary[word] = len(dictionary) #word to key\n",
    "    reverse = dict(zip(dictionary.values(), dictionary.keys())) #key to word\n",
    "    return dictionary, reverse\n",
    "\n",
    "def punctuations():\n",
    "    return {\n",
    "        '.': '||period||',\n",
    "        ',': '||comma||',\n",
    "        '\"': '||quotes||',\n",
    "        ';': '||semicolon||',\n",
    "        '!': '||exclamation-mark||',\n",
    "        '?': '||question-mark||',\n",
    "        '(': '||left-parentheses||',\n",
    "        ')': '||right-parentheses||',\n",
    "        '--': '||emm-dash||',\n",
    "        '\\n': '||return||'  \n",
    "    }\n",
    "\n",
    "tokens = punctuations()\n",
    "for token in tokens:\n",
    "    corpus = corpus.replace(token, \" \" + tokens[token] + \" \")\n",
    "corpus = corpus.lower()\n",
    "corpus = corpus.split()\n",
    "\n",
    "dictionary, reverse = create_tables(corpus)\n",
    "\n",
    "def make_minibatches(text, batch_size, sequence_length):\n",
    "    words = batch_size * sequence_length\n",
    "    num_batches = len(text) // words\n",
    "    text = text[:num_batches*words]\n",
    "    y = np.array(text[1:] + [text[0]])\n",
    "    x = np.array(text)\n",
    "    x_batches = np.split(x.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    y_batches = np.split(y.reshape(batch_size, -1), num_batches, axis=1)\n",
    "    \n",
    "    return np.array(list(zip(x_batches, y_batches)))\n",
    "\n",
    "#Hyperparameters\n",
    "\n",
    "epochs = 100\n",
    "batch_size = 512\n",
    "rnn_size = 512\n",
    "num_layers = 3\n",
    "keep_prob = 0.7 #dropout rate\n",
    "embed_dim = 512\n",
    "sequence_length = 30\n",
    "lr = 0.001\n",
    "\n",
    "save_dir = \"./output\"\n",
    "\n",
    "training = tf.Graph()\n",
    "with training.as_default():\n",
    "\n",
    "    input_text = tf.placeholder(tf.int32, [None, None], name=\"input\")\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name=\"targets\")\n",
    "    alpha = tf.placeholder(tf.float32, name='alpha')\n",
    "\n",
    "    num_words = len(dictionary)\n",
    "    input_shape = tf.shape(input_text)\n",
    "\n",
    "    rnn_layers = []\n",
    "    for i in range(num_layers):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_size)\n",
    "        drop_cell = tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "        rnn_layers.append(drop_cell)\n",
    "    cell = tf.contrib.rnn.MultiRNNCell(rnn_layers)\n",
    "\n",
    "    initial_state = cell.zero_state(input_shape[0], tf.float32)\n",
    "    initial_state = tf.identity(initial_state, name='initial_state')\n",
    "\n",
    "    embed = tf.contrib.layers.embed_sequence(input_text, num_words, embed_dim)\n",
    "\n",
    "    outputs, final_state = tf.nn.dynamic_rnn(cell, embed, dtype=tf.float32)\n",
    "    final_state = tf.identity(final_state, name='final_state')\n",
    "\n",
    "    logits = tf.contrib.layers.fully_connected(outputs, num_words, activation_fn=None)\n",
    "\n",
    "    probs = tf.nn.softmax(logits, name='probs')\n",
    "\n",
    "    cost = tf.contrib.seq2seq.sequence_loss(\n",
    "        logits,\n",
    "        targets,\n",
    "        tf.ones([input_shape[0], input_shape[1]])\n",
    "    )\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(alpha)\n",
    "\n",
    "    gradients = optimizer.compute_gradients(cost)\n",
    "    capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "    train_op = optimizer.apply_gradients(capped_gradients)\n",
    "\n",
    "\n",
    "corpus_int = [dictionary[word] for word in corpus]\n",
    "batches = make_minibatches(corpus_int, batch_size, sequence_length)\n",
    "\n",
    "with tf.Session(graph=training) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        state = sess.run(initial_state, {input_text: batches[0][0]})\n",
    "        print(\"Epoch \" + str(epoch))\n",
    "        for batch_index, (x, y) in enumerate(batches):\n",
    "            feed_dict = {\n",
    "                input_text: x,\n",
    "                targets: y,\n",
    "                initial_state: state,\n",
    "                alpha: lr\n",
    "            }\n",
    "            train_loss, state, _ = sess.run([cost, final_state, train_op], feed_dict)\n",
    "\n",
    "        if epoch % 25 == 0:\n",
    "            saver = tf.train.Saver()\n",
    "            saver.save(sess, save_dir)\n",
    "            print(\"model saved\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
